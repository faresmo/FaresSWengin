{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI for SWEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVP\n",
    "- Query docs you have in a `/data` folder:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"what is o1\")\n",
    "print(response)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is essentially a RAG system that abstracts over\n",
    "\n",
    "- Embeddings,\n",
    "- Vector stores, and\n",
    "- Chunking.\n",
    "\n",
    "If this doesn't mean much to you yet, that's cool and you've come to the right place! By the end of this workshop, you'll know about all of this stuff.\n",
    "\n",
    "The most important aspect to recognize is that we're able to \n",
    "- feed an LLM our own documents,\n",
    "- query them,\n",
    "- and get a generative response.\n",
    "\n",
    "It's pretty incredible that we can do all of this in 5 lines of code but we need to make sure we don't enter POC purgatory. Let's keep building!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Say something briefly about this image:]\n",
    "![Alt text](img/0-simple-RAG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Front End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `2-app-front-end.py` and you'll see something like this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](img/1-gradio-fe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now drop a PDF in and query it. In order to prepare for the rest of the workshop, we encourage you to turn your LinkedIn profile into a PDF (print --> PDF) and query your own professional profile!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build this basic app, we've used\n",
    "\n",
    "- Gradio in the front end,\n",
    "- PyMuPDF to help Python read the PDF, and\n",
    "- Refactored our code.\n",
    "\n",
    "[say more about all of these things]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[write what type of questions to ask to prep for the app we build later :) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to use a local model [talk about the reasons]. There are many ways to use local models and Ollama is a popular one.\n",
    "If you have Ollama installed and running, it's relatively straightforward to switch out the OpenAI model above for any model you have locally and can run with Ollama. The first thing to do is to switch out the default model that the LlamaIndex query engine uses (OAI) for your local model:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "# Initialize the Ollama LLM with the desired model (e.g., LLaMA2)\n",
    "llm = Ollama(model=\"llama2\", request_timeout=60.0)\n",
    "# Set up the query engine with the Ollama LLM\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you may think this is enough, but it isn't quite! The following won't work [insert error msg here?]:\n",
    "\n",
    "```\n",
    "def process_pdf(pdf_file):\n",
    "    extracted_text = extract_text_from_pdf(pdf_file)  # Extract text from the uploaded PDF\n",
    "    document = Document(text=extracted_text)  # Create a proper Document object\n",
    "    index = VectorStoreIndex.from_documents([document])  # Create index from document\n",
    "    return index\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because, when using the default OpenAI model, it automatically uses a particular OpenAI model to embed your documents in your vector store.\n",
    "\n",
    "Now we're using a local model, we need to specify precisely which embedding model we want to use, such as\n",
    "\n",
    "```\n",
    "def process_pdf(pdf_file):\n",
    "    extracted_text = extract_text_from_pdf(pdf_file)\n",
    "    document = Document(text=extracted_text)\n",
    "    # Specify a Hugging Face model for local embeddings\n",
    "    embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    index = VectorStoreIndex.from_documents([document], embed_model=embed_model)\n",
    "    return index\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatting with your PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the time, we'd like to ask a clarifying question [give example].\n",
    "The system we've just built does not store conversation history, meaning it has no memory, so the next step is to make it have this.\n",
    "There are lots of clever ways to give an LLM enough context to know the history of the conversation (and this is currently a *very* active area of research). The most naive way is to prepend the entire conversation to the query as follows:\n",
    "\n",
    "```\n",
    "# Add previous conversation to the query for context\n",
    "conversation = \"\\n\".join([f\"User: {h[0]}\\nAssistant: {h[1]}\" for h in history])\n",
    "conversation += f\"\\nUser: {query}\\n\"\n",
    "\n",
    "# Query the index using the user's question with context\n",
    "response = query_engine.query(conversation)\n",
    "\n",
    "history.append((query, response.response))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've done this in `4-app-convo.py`. Let's now see it in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging Conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
